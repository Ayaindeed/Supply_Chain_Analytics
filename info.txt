Architecture de Business Intelligence pour l'Optimisation de ChaÃ®ne Logistique : ETL OrchestrÃ©, PrÃ©diction de Demande et Dashboard DÃ©cisionnele
dbt + Airflow - RÃ´le de dbt dans ton Projet
RÃ‰SUMÃ‰ : Quel rÃ´le joue dbt ?
dbt n'est PAS un orchÃ©strateur (c'est Airflow) dbt n'est PAS un outil d'extraction (c'est Python/Airflow)
dbt EST un outil de TRANSFORMATION SQL spÃ©cialisÃ©

ğŸ“Š LA NOUVELLE ARCHITECTURE
SOURCES (CSV)
    â†“
[AIRFLOW - ORCHESTRATION]
    â”œâ”€ Task 1: Extract (Python/Pandas)
    â”‚   â†’ Lire CSV, charger dans PostgreSQL (raw_data)
    â”‚
    â”œâ”€ Task 2: dbt Transform (SQL)
    â”‚   â†’ Nettoyer, transformer, crÃ©er vues
    â”‚   â†’ GÃ©rÃ©e par Airflow
    â”‚
    â”œâ”€ Task 3: Feature Engineering (Python/dbt)
    â”‚   â†’ CrÃ©er colonnes dÃ©rivÃ©es
    â”‚
    â”œâ”€ Task 4: Data Quality Tests (dbt)
    â”‚   â†’ Tests automatiques
    â”‚
    â””â”€ Task 5: ML Modeling (Python/XGBoost)
        â†’ EntraÃ®ner modÃ¨les
    â†“
[PostgreSQL - DATA WAREHOUSE]
    â”œâ”€ raw_data (donnÃ©es brutes)
    â”œâ”€ staging (donnÃ©es transformÃ©es)
    â””â”€ analytics (vues finales pour BI)
    â†“
[Power BI - REPORTING]
    â””â”€ Dashboard interactive


ğŸ”„ LE FLUX COMPLET
Sans dbt (Avant) :
CSV â†’ Python (Pandas) â†’ SQL (Postgres) â†’ Power BI
     (complexe, peu testable)

Avec dbt (AprÃ¨s) :
CSV â†’ Airflow (Orchestration)
         â†“
      dbt (Transformations SQL + Tests)
         â†“
      PostgreSQL (Clean Data)
         â†“
      Power BI (Dashboard)
     (modulaire, testable, documentÃ©)


ğŸ¯ LES 5 RÃ”LES DE dbt
1ï¸âƒ£ TRANSFORMATION SQL (RÃ´le Principal)
Avant (sans dbt) - Code Python complexe :
# scripts/transform.py
def transform_data(df):
    df['Delai_livraison'] = (df['Delivery Date'] - df['Order Date']).dt.days
    df['Mois'] = df['Order Date'].dt.month
    df['Is_Late'] = (df['Delai_livraison'] > 7).astype(int)
    # ... 100 lignes de pandas compliquÃ©
    return df

AprÃ¨s (avec dbt) - SQL lisible :
-- dbt/models/staging/stg_orders.sql
SELECT 
    order_id,
    order_date,
    delivery_date,
    DATE_DIFF(delivery_date, order_date) as delai_livraison,
    EXTRACT(MONTH FROM order_date) as mois,
    CASE WHEN DATE_DIFF(delivery_date, order_date) > 7 THEN 1 ELSE 0 END as is_late
FROM {{ source('raw', 'orders') }}
WHERE order_date IS NOT NULL

Avantages : âœ… SQL > Pandas pour la transformation âœ… Version contrÃ´lÃ©e (Git) âœ… DocumentÃ© automatiquement âœ… RÃ©utilisable

2ï¸âƒ£ GESTION DES DÃ‰PENDANCES
dbt gÃ¨re automatiquement l'ordre d'exÃ©cution :
-- dbt/models/staging/stg_orders.sql
SELECT * FROM {{ source('raw', 'raw_data') }}

-- dbt/models/staging/stg_deliveries.sql
SELECT * FROM {{ source('raw', 'deliveries') }}

-- dbt/models/marts/fct_supply_chain.sql
SELECT 
    o.*,
    d.delai
FROM {{ ref('stg_orders') }} o
LEFT JOIN {{ ref('stg_deliveries') }} d USING (order_id)
-- dbt sait que celui-ci dÃ©pend des 2 autres â†‘

Sans dbt, tu dois gÃ©rer manuellement avec Airflow :
task_1_orders >> task_2_deliveries >> task_3_combine

Avec dbt, c'est automatique :
dbt run  # ExÃ©cute dans le bon ordre automatiquement!


3ï¸âƒ£ TESTS DE QUALITÃ‰ DE DONNÃ‰ES
# dbt/models/staging/stg_orders.yml
models:
  - name: stg_orders
    columns:
      - name: order_id
        tests:
          - unique           # Chaque ID est unique
          - not_null         # Pas de NULL
      - name: order_date
        tests:
          - not_null
      - name: is_late
        tests:
          - accepted_values: # Seulement 0 ou 1
              values: [0, 1]

  - name: fct_supply_chain
    tests:
      - dbt_utils.recency:  # Les donnÃ©es ne sont pas trop vieilles
          datepart: day
          interval: 7
          field: order_date

ExÃ©cution :
dbt test  # Tous les tests s'exÃ©cutent

# RÃ©sultat :
âœ… Unique test on stg_orders.order_id: PASSED
âœ… Not null test on stg_orders.order_date: PASSED
âŒ Accepted values test on fct_supply_chain.is_late: FAILED
   â†’ 5 rows have invalid values

IntÃ©gration Airflow :
@task
def run_dbt_tests():
    result = os.system('dbt test')
    if result != 0:
        raise ValueError("dbt tests failed!")
    return "Tests passed"


4ï¸âƒ£ DOCUMENTATION AUTOMATIQUE
dbt gÃ©nÃ¨re automatiquement de la documentation :
dbt docs generate
dbt docs serve  # Ouvre http://localhost:8000

RÃ©sultat : Une documentation interactive avec :
SchÃ©ma de chaque table
DÃ©pendances visuelles
Descriptions (depuis YAML)
Historique des donnÃ©es
Sans dbt, tu fais Ã§a manuellement (100% pas fait en rÃ©alitÃ© ğŸ˜…)

5ï¸âƒ£ INCREMENTAL BUILDS (TrÃ¨s efficace!)
-- dbt/models/marts/fct_daily_metrics.sql
{{ config(
    materialized='incremental',
    unique_key='order_id',
    on_schema_change='fail'
) }}

SELECT 
    DATE(order_date) as order_day,
    COUNT(*) as nb_orders,
    SUM(sales) as ca_day
FROM {{ ref('stg_orders') }}

{% if execute %}
    {% if this.exists %}
        WHERE order_date > (SELECT MAX(order_date) FROM {{ this }})
    {% endif %}
{% endif %}

GROUP BY order_day

RÃ©sultat :
1Ã¨re exÃ©cution : Charge TOUS les donnÃ©es
2e exÃ©cution : Ajoute SEULEMENT les nouvelles donnÃ©es
Temps : 100x plus rapide! âš¡

ğŸš€ ARCHITECTURE FINALE : Airflow + dbt + PostgreSQL
Structure de Projet dbt
supply_chain_project/
â”œâ”€â”€ dbt/
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ staging/
â”‚   â”‚   â”‚   â”œâ”€â”€ stg_orders.sql          # Orders bruts nettoyÃ©s
â”‚   â”‚   â”‚   â”œâ”€â”€ stg_deliveries.sql      # Livraisons nettoyÃ©es
â”‚   â”‚   â”‚   â””â”€â”€ stg_products.sql        # Produits nettoyÃ©s
â”‚   â”‚   â”œâ”€â”€ intermediate/
â”‚   â”‚   â”‚   â”œâ”€â”€ int_orders_with_delay.sql
â”‚   â”‚   â”‚   â””â”€â”€ int_products_metrics.sql
â”‚   â”‚   â””â”€â”€ marts/
â”‚   â”‚       â”œâ”€â”€ fct_supply_chain.sql    # Table de fait
â”‚   â”‚       â”œâ”€â”€ dim_products.sql        # Dimension produits
â”‚   â”‚       â”œâ”€â”€ dim_suppliers.sql       # Dimension fournisseurs
â”‚   â”‚       â””â”€â”€ analytics/
â”‚   â”‚           â”œâ”€â”€ vw_kpi_global.sql
â”‚   â”‚           â”œâ”€â”€ vw_perf_region.sql
â”‚   â”‚           â””â”€â”€ vw_tendance.sql
â”‚   â”œâ”€â”€ tests/
â”‚   â”‚   â”œâ”€â”€ generic/
â”‚   â”‚   â”‚   â””â”€â”€ recency.sql
â”‚   â”‚   â””â”€â”€ singular/
â”‚   â”‚       â””â”€â”€ test_no_future_dates.sql
â”‚   â”œâ”€â”€ dbt_project.yml
â”‚   â”œâ”€â”€ profiles.yml
â”‚   â””â”€â”€ packages.yml
â”‚
â”œâ”€â”€ airflow/
â”‚   â””â”€â”€ dags/
â”‚       â””â”€â”€ supply_chain_dbt_dag.py
â”‚
â””â”€â”€ data/
    â””â”€â”€ raw/
        â””â”€â”€ DataCoSupplyChain.csv

DAG Airflow avec dbt
# airflow/dags/supply_chain_dbt_dag.py

from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.operators.bash import BashOperator
from airflow.providers.dbt.cloud.operators.dbt import DbtCloudRunJobOperator
from datetime import datetime, timedelta

default_args = {
    'owner': 'data_team',
    'start_date': datetime(2024, 1, 1),
    'retries': 2,
    'retry_delay': timedelta(minutes=5)
}

dag = DAG(
    'supply_chain_dbt_pipeline',
    default_args=default_args,
    description='ETL avec dbt + Airflow',
    schedule_interval='0 2 * * *',  # Daily 2 AM
    catchup=False
)

# ===== TASK 1: EXTRACT =====
def extract_csv_to_postgres():
    """Extraire CSV et charger dans PostgreSQL (raw_data)"""
    import pandas as pd
    from sqlalchemy import create_engine
    
    # Lire CSV
    df = pd.read_csv('data/raw/DataCoSupplyChain.csv', encoding='latin-1')
    
    # Charger dans PostgreSQL
    engine = create_engine(
        'postgresql://datateam:SecurePass123!@postgres:5432/supply_chain_dw'
    )
    df.to_sql('raw_data', engine, schema='raw_data', if_exists='append')
    
    return f"Loaded {len(df)} rows"

task_extract = PythonOperator(
    task_id='extract_csv',
    python_callable=extract_csv_to_postgres,
    dag=dag
)

# ===== TASK 2: dbt TRANSFORM & TESTS =====
# Option A: Utiliser dbt CLI directement
task_dbt_run = BashOperator(
    task_id='dbt_run',
    bash_command='cd /app/dbt && dbt run --profiles-dir .',
    dag=dag
)

task_dbt_test = BashOperator(
    task_id='dbt_test',
    bash_command='cd /app/dbt && dbt test --profiles-dir .',
    dag=dag
)

# Option B: Utiliser dbt Cloud (si vous avez un compte)
# task_dbt_run = DbtCloudRunJobOperator(
#     task_id='dbt_run_cloud',
#     job_id=123456,
#     dbt_cloud_conn_id='dbt_cloud_api',
#     dag=dag
# )

# ===== TASK 3: FEATURE ENGINEERING =====
def feature_engineering():
    """CrÃ©er des features supplÃ©mentaires"""
    import pandas as pd
    from sqlalchemy import create_engine
    
    engine = create_engine(
        'postgresql://datateam:SecurePass123!@postgres:5432/supply_chain_dw'
    )
    
    # Lire depuis dbt staging
    df = pd.read_sql(
        "SELECT * FROM analytics.stg_orders",
        engine
    )
    
    # Ajouter features
    df['revenue_per_day'] = df['sales'] / (df['delai_livraison'] + 1)
    df['is_high_value'] = (df['sales'] > df['sales'].quantile(0.75)).astype(int)
    
    # Sauvegarder
    df.to_sql('features_orders', engine, schema='staging', if_exists='replace')
    
    return "Features created"

task_features = PythonOperator(
    task_id='create_features',
    python_callable=feature_engineering,
    dag=dag
)

# ===== TASK 4: ML MODELING =====
def train_demand_model():
    """EntraÃ®ner modÃ¨le XGBoost"""
    import pandas as pd
    import xgboost as xgb
    from sklearn.model_selection import train_test_split
    from sqlalchemy import create_engine
    
    engine = create_engine(
        'postgresql://datateam:SecurePass123!@postgres:5432/supply_chain_dw'
    )
    
    # Lire features
    df = pd.read_sql("SELECT * FROM staging.features_orders", engine)
    
    X = df[['delai_livraison', 'mois', 'is_high_value']]
    y = df['sales']
    
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)
    
    model = xgb.XGBRegressor(n_estimators=100, max_depth=5)
    model.fit(X_train, y_train)
    
    score = model.score(X_test, y_test)
    print(f"Model RÂ²: {score:.4f}")
    
    # Sauvegarder prÃ©dictions
    predictions = model.predict(df[['delai_livraison', 'mois', 'is_high_value']])
    df['predicted_sales'] = predictions
    df.to_sql('predictions', engine, schema='analytics', if_exists='replace')
    
    return f"Model trained with RÂ²={score:.4f}"

task_model = PythonOperator(
    task_id='train_model',
    python_callable=train_demand_model,
    dag=dag
)

# ===== DEPENDENCIES =====
task_extract >> task_dbt_run >> task_dbt_test >> task_features >> task_model


ğŸ“Š dbt.yml Configuration
# dbt/dbt_project.yml
name: 'supply_chain'
version: '1.0.0'
config-version: 2

profile: 'supply_chain'

model-paths: ["models"]
test-paths: ["tests"]
data-paths: ["data"]
analysis-paths: ["analysis"]
macro-paths: ["macros"]
docs-paths: ["docs"]

vars:
  execution_date: '{{ run_started_at }}'
  model_version: 'v1'

# dbt/profiles.yml
supply_chain:
  outputs:
    dev:
      type: postgres
      host: postgres
      user: datateam
      password: SecurePass123!
      port: 5432
      dbname: supply_chain_dw
      schema: analytics
      threads: 4
      keepalives_idle: 0

    prod:
      type: postgres
      host: postgres
      user: datateam
      password: SecurePass123!
      port: 5432
      dbname: supply_chain_dw
      schema: analytics
      threads: 8
      keepalives_idle: 0

  target: dev


âœ… AVANTAGES DE dbt + Airflow
Aspect
Sans dbt
Avec dbt
Transformation
Python complexe
SQL lisible
Tests
Manuel
Automatique
Documentation
Manuelle (jamais fait)
Auto-gÃ©nÃ©rÃ©e
RÃ©utilisabilitÃ©
Difficile
Via ref()
Version Control
Code Pandas
SQL + Git
Performance
Pandas lent
DB compute rapide
Debugging
Difficile
dbt lineage
DÃ©pendances
Manuel avec Airflow
Auto avec dbt


ğŸ¯ QUAND UTILISER QUOI
âœ… Utilise dbt pour :
âœ… Transformation de donnÃ©es (SQL)
âœ… CrÃ©er des vues/tables
âœ… Tests de qualitÃ©
âœ… Documentation
âœ… Modulariser le code
âœ… Utilise Airflow pour :
âœ… Orchestrer le flux global
âœ… Extraction de donnÃ©es
âœ… Machine Learning
âœ… Alertes
âœ… Scheduling complexe
âŒ Ã‰vite :
âŒ dbt pour l'extraction (c'est Airflow/Python)
âŒ Airflow pour les transformations SQL (c'est dbt)

ğŸ“¦ Installation dbt
# 1. Installer dbt pour PostgreSQL
pip install dbt-postgres

# 2. Initialiser projet dbt
dbt init supply_chain --profiles-dir .

# 3. Configurer profiles.yml (fait ci-dessus)

# 4. Tester connexion
dbt debug

# 5. ExÃ©cuter
dbt run      # ExÃ©cuter les modÃ¨les
dbt test     # ExÃ©cuter les tests
dbt docs generate  # GÃ©nÃ©rer docs


ğŸš€ RÃ‰SUMÃ‰ FINAL
AIRFLOW = OrchÃ©strateur (OÃ¹? Quand? Quoi faire si erreur?)
dbt = Transformation (Comment transformer les donnÃ©es?)
PostgreSQL = Data Warehouse (OÃ¹ stocker?)
Power BI = Reporting (Comment visualiser?)

Ton pipeline quotidien :
Airflow dÃ©clenche Ã  2 AM
Extrait CSV â†’ PostgreSQL (Airflow)
dbt transforme dans PostgreSQL (dbt)
dbt teste les donnÃ©es (dbt)
Python crÃ©e features (Airflow)
XGBoost entraÃ®ne modÃ¨les (Airflow)
Power BI lit les vues dbt finales
Dashboard mis Ã  jour âœ…
